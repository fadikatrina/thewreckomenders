{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from ast import literal_eval\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in our datasets, consisting of user data and data for the recipes. We will be using the ratings that users gave in combination with the recipe's ingredients, techniques and calories to perform recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_users = pd.read_csv('Data/PP_users.csv')\n",
    "#Note that there are no duplicates\n",
    "df_users.head()\n",
    "\n",
    "# u = user_id, \n",
    "# techniques = techniques used for items  that were interacted with(index is a technique with the number being a counter),\n",
    "# items = item_ids of items that were interacted with, \n",
    "# n_items = number of items reviewed, \n",
    "# ratings = ratings for items reviewed, \n",
    "# n_ratings = number of ratings\n",
    "\n",
    "number_of_users_with_less_than_10_reviews = df_users[df_users[\"n_ratings\"] < 10].shape[0]\n",
    "number_of_users_with_less_than_5_reviews = df_users[df_users[\"n_ratings\"] < 5].shape[0]\n",
    "number_of_users = len(df_users.index)\n",
    "\n",
    "print(str(round((number_of_users_with_less_than_10_reviews / number_of_users) * 100,\n",
    "                1)) + \"% of user have less than 10 reviews\")\n",
    "print(str(round((number_of_users_with_less_than_5_reviews / number_of_users) * 100,\n",
    "                1)) + \"% of user have less than 5 reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_recipes = pd.read_csv('Data/PP_recipes.csv')\n",
    "#Note that there are no duplicates\n",
    "df_recipes.head()\n",
    "\n",
    "# id = recipe_id, i = Recipe ID mapped to contiguous integers from 0, \n",
    "# name_tokes = BPE-tokenized recipe name,\n",
    "# ingredient_tokens = BPE-tokenized ingredients list (list of lists), \n",
    "# steps_tokens = BPE-tokenized steps, \n",
    "# techniques = List of techniques used in recipe,\n",
    "# calorie_level = either a 0, 1 or 2 indicating how much calories it contains,\n",
    "# ingredient_ids = the ids of the ingredients used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv('Data/PP_users.csv')\n",
    "df_users.drop('techniques', axis=1, inplace=True)\n",
    "df_users.drop('n_items', axis=1, inplace=True)\n",
    "df_users.drop('n_ratings', axis=1, inplace=True)\n",
    "\n",
    "df_users = df_users.rename(columns={'u': 'user', 'items': 'item', 'ratings': 'rating'})\n",
    "\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to make the explode function, source: https://stackoverflow.com/questions/63472664/pandas-explode-function-not-working-for-list-of-string-column\n",
    "df_users['rating'] = df_users['rating'].apply(literal_eval)\n",
    "df_users['item'] = df_users['item'].apply(literal_eval)\n",
    "\n",
    "df_users = df_users.explode(['rating', 'item'], ignore_index=True)\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes.drop('i', axis=1, inplace=True)\n",
    "df_recipes.drop('name_tokens', axis=1, inplace=True)\n",
    "df_recipes.drop('ingredient_tokens', axis=1, inplace=True)\n",
    "df_recipes.drop('steps_tokens', axis=1, inplace=True)\n",
    "\n",
    "df_recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to make the explode function, source: https://stackoverflow.com/questions/63472664/pandas-explode-function-not-working-for-list-of-string-column\n",
    "df_recipes['techniques'] = df_recipes['techniques'].apply(literal_eval)\n",
    "df_recipes['ingredient_ids'] = df_recipes['ingredient_ids'].apply(literal_eval)\n",
    "\n",
    "df_recipes = df_recipes.explode('techniques')\n",
    "df_recipes = df_recipes.explode('ingredient_ids')\n",
    "df_recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are we removing users and recipes with few ratings?\n",
    "# Do we still need the explode for anything?\n",
    "# Should we change the features that we're using?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this file is made by us to only have to load nutritional info instead of a giant csv file\n",
    "nutrition_data = pd.read_csv('Data/nutrition.csv', sep=';')\n",
    "nutrition_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to give the user a (tweakable) health filter, so they can filter their suggestions to only contain healthy recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions returns the recipe_ids\n",
    "# Note: limits takes max sugar amount, max sodium, min protein amount, max saturated_fat (not in grams, but percentage of nutritional content)\n",
    "def ApplyHealthFilter(recipe_ids, limits=[.15, .35, .10, .25], debug_prints=False):\n",
    "    healthy_recipes = []\n",
    "\n",
    "    # Get the nutritional information for the relevant recipes\n",
    "    recipes = nutrition_data.loc[nutrition_data['id'].isin(recipe_ids)]\n",
    "\n",
    "    for index, recipe in recipes.iterrows():\n",
    "        # Nutrition information in calories, total fat, sugar, sodium, protein, saturated fat, carbohydrates \n",
    "        nutrition_values = recipe['nutrition']\n",
    "        # convert string version of array into a proper array\n",
    "        nutrition_values = literal_eval(nutrition_values)\n",
    "\n",
    "        sugar = nutrition_values[2]\n",
    "        sodium = nutrition_values[3]\n",
    "        protein = nutrition_values[4]\n",
    "        saturated_fat = nutrition_values[5]\n",
    "\n",
    "        # Since the nutritional info is in absolute numbers instead of per 100 grams, we'll normalize\n",
    "        normalization_factor = sum(nutrition_values[1:])\n",
    "        normalization_factor = max(normalization_factor, 0.01)\n",
    "\n",
    "        sugar /= normalization_factor\n",
    "        sodium /= normalization_factor\n",
    "        protein /= normalization_factor\n",
    "        saturated_fat /= normalization_factor\n",
    "\n",
    "        if sugar < limits[0] and sodium < limits[1] and protein > limits[2] and saturated_fat < limits[3]:\n",
    "            healthy_recipes.append(recipe['id'])\n",
    "\n",
    "            if debug_prints:\n",
    "                print(\"Healthy: \", index)\n",
    "\n",
    "        else:\n",
    "            if debug_prints:\n",
    "                print(\"Unhealthy: \", index, sugar, sodium, protein, saturated_fat)\n",
    "\n",
    "    return healthy_recipes\n",
    "\n",
    "\n",
    "df_random = df_users.sample(n=20)\n",
    "recipe_ids = df_random['item']\n",
    "# limits = [.15, .35, .10, .25]\n",
    "\n",
    "healhty_recipes = ApplyHealthFilter(recipe_ids)\n",
    "print(healhty_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a lot of high dimensional data, we could make use of SVD to speed up computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>137739</td>\n",
       "      <td>55</td>\n",
       "      <td>47892</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['make a choice and proceed with recipe', 'dep...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bit different  breakfast pizza</td>\n",
       "      <td>31490</td>\n",
       "      <td>30</td>\n",
       "      <td>26278</td>\n",
       "      <td>2002-06-17</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>['preheat oven to 425 degrees f', 'press dough...</td>\n",
       "      <td>this recipe calls for the crust to be prebaked...</td>\n",
       "      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all in the kitchen  chili</td>\n",
       "      <td>112140</td>\n",
       "      <td>130</td>\n",
       "      <td>196586</td>\n",
       "      <td>2005-02-25</td>\n",
       "      <td>['time-to-make', 'course', 'preparation', 'mai...</td>\n",
       "      <td>[269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>['brown ground beef in large pot', 'add choppe...</td>\n",
       "      <td>this modified version of 'mom's' chili was a h...</td>\n",
       "      <td>['ground beef', 'yellow onions', 'diced tomato...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alouette  potatoes</td>\n",
       "      <td>59389</td>\n",
       "      <td>45</td>\n",
       "      <td>68585</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['place potatoes in a large pot of lightly sal...</td>\n",
       "      <td>this is a super easy, great tasting, make ahea...</td>\n",
       "      <td>['spreadable cheese with garlic and herbs', 'n...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amish  tomato ketchup  for canning</td>\n",
       "      <td>44061</td>\n",
       "      <td>190</td>\n",
       "      <td>41706</td>\n",
       "      <td>2002-10-25</td>\n",
       "      <td>['weeknight', 'time-to-make', 'course', 'main-...</td>\n",
       "      <td>[352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]</td>\n",
       "      <td>5</td>\n",
       "      <td>['mix all ingredients&amp; boil for 2 1 / 2 hours ...</td>\n",
       "      <td>my dh's amish mother raised him on this recipe...</td>\n",
       "      <td>['tomato juice', 'apple cider vinegar', 'sugar...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name      id  minutes  \\\n",
       "0  arriba   baked winter squash mexican style  137739       55   \n",
       "1            a bit different  breakfast pizza   31490       30   \n",
       "2                   all in the kitchen  chili  112140      130   \n",
       "3                          alouette  potatoes   59389       45   \n",
       "4          amish  tomato ketchup  for canning   44061      190   \n",
       "\n",
       "   contributor_id   submitted  \\\n",
       "0           47892  2005-09-16   \n",
       "1           26278  2002-06-17   \n",
       "2          196586  2005-02-25   \n",
       "3           68585  2003-04-14   \n",
       "4           41706  2002-10-25   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "1  ['30-minutes-or-less', 'time-to-make', 'course...   \n",
       "2  ['time-to-make', 'course', 'preparation', 'mai...   \n",
       "3  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "4  ['weeknight', 'time-to-make', 'course', 'main-...   \n",
       "\n",
       "                                    nutrition  n_steps  \\\n",
       "0       [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
       "1   [173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]        9   \n",
       "2  [269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]        6   \n",
       "3   [368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]       11   \n",
       "4   [352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]        5   \n",
       "\n",
       "                                               steps  \\\n",
       "0  ['make a choice and proceed with recipe', 'dep...   \n",
       "1  ['preheat oven to 425 degrees f', 'press dough...   \n",
       "2  ['brown ground beef in large pot', 'add choppe...   \n",
       "3  ['place potatoes in a large pot of lightly sal...   \n",
       "4  ['mix all ingredients& boil for 2 1 / 2 hours ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  autumn is my favorite time of year to cook! th...   \n",
       "1  this recipe calls for the crust to be prebaked...   \n",
       "2  this modified version of 'mom's' chili was a h...   \n",
       "3  this is a super easy, great tasting, make ahea...   \n",
       "4  my dh's amish mother raised him on this recipe...   \n",
       "\n",
       "                                         ingredients  n_ingredients  \n",
       "0  ['winter squash', 'mexican seasoning', 'mixed ...              7  \n",
       "1  ['prepared pizza crust', 'sausage patty', 'egg...              6  \n",
       "2  ['ground beef', 'yellow onions', 'diced tomato...             13  \n",
       "3  ['spreadable cheese with garlic and herbs', 'n...             11  \n",
       "4  ['tomato juice', 'apple cider vinegar', 'sugar...              8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_data = pd.read_csv('Data/RAW_recipes.csv', sep=',')\n",
    "recipe_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes = recipe_data['minutes']\n",
    "tags = recipe_data['tags']\n",
    "nutrition = recipe_data['nutrition']\n",
    "steps = recipe_data['steps']\n",
    "ingredients = recipe_data['ingredients']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231637, 593)\n",
      "(231637, 10000)\n",
      "(231637, 4213)\n"
     ]
    }
   ],
   "source": [
    "# Transform text into numerical data\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tags = vectorizer.fit_transform(tags)\n",
    "steps = vectorizer.fit_transform(steps)\n",
    "ingredients = vectorizer.fit_transform(ingredients)\n",
    "\n",
    "print(tags.shape)\n",
    "print(steps.shape)\n",
    "print(ingredients.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(minutes)\n",
    "# print(tags)\n",
    "# print(nutrition)\n",
    "# print(steps)\n",
    "# print(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steem\\anaconda3\\envs\\RecommenderProject\\lib\\site-packages\\sklearn\\utils\\validation.py:738: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0          55\n 1          30\n 2         130\n 3          45\n 4         190\n          ...\n 231632     60\n 231633      5\n 231634     40\n 231635     29\n 231636     20\n Name: minutes, Length: 231637, dtype: int64\n <231637x593 sparse matrix of type '<class 'numpy.float64'>'\n \twith 6351801 stored elements in Compressed Sparse Row format>\n 0               [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]\n 1           [173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]\n 2          [269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]\n 3           [368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]\n 4           [352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]\n                              ...\n 231632    [415.2, 26.0, 34.0, 26.0, 44.0, 21.0, 15.0]\n 231633          [14.8, 0.0, 2.0, 58.0, 1.0, 0.0, 1.0]\n 231634           [59.2, 6.0, 2.0, 3.0, 6.0, 5.0, 0.0]\n 231635      [188.0, 11.0, 57.0, 11.0, 7.0, 21.0, 9.0]\n 231636       [174.9, 14.0, 33.0, 4.0, 4.0, 11.0, 6.0]\n Name: nutrition, Length: 231637, dtype: object\n <231637x10000 sparse matrix of type '<class 'numpy.float64'>'\n \twith 13878050 stored elements in Compressed Sparse Row format>\n <231637x4213 sparse matrix of type '<class 'numpy.float64'>'\n \twith 3612449 stored elements in Compressed Sparse Row format>].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14040/74332005.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecommenderProject\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \"\"\"\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecommenderProject\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mReduced\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mof\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mwill\u001b[0m \u001b[0malways\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdense\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \"\"\"\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecommenderProject\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecommenderProject\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    762\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0          55\n 1          30\n 2         130\n 3          45\n 4         190\n          ...\n 231632     60\n 231633      5\n 231634     40\n 231635     29\n 231636     20\n Name: minutes, Length: 231637, dtype: int64\n <231637x593 sparse matrix of type '<class 'numpy.float64'>'\n \twith 6351801 stored elements in Compressed Sparse Row format>\n 0               [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]\n 1           [173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]\n 2          [269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]\n 3           [368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]\n 4           [352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]\n                              ...\n 231632    [415.2, 26.0, 34.0, 26.0, 44.0, 21.0, 15.0]\n 231633          [14.8, 0.0, 2.0, 58.0, 1.0, 0.0, 1.0]\n 231634           [59.2, 6.0, 2.0, 3.0, 6.0, 5.0, 0.0]\n 231635      [188.0, 11.0, 57.0, 11.0, 7.0, 21.0, 9.0]\n 231636       [174.9, 14.0, 33.0, 4.0, 4.0, 11.0, 6.0]\n Name: nutrition, Length: 231637, dtype: object\n <231637x10000 sparse matrix of type '<class 'numpy.float64'>'\n \twith 13878050 stored elements in Compressed Sparse Row format>\n <231637x4213 sparse matrix of type '<class 'numpy.float64'>'\n \twith 3612449 stored elements in Compressed Sparse Row format>].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# X contains minutes to cook, tags, nutritional values, cooking steps, ingredients\n",
    "# X = [minutes, tags, nutrition, steps, ingredients]\n",
    "\n",
    "# svd = TruncatedSVD(n_components=100, n_iter=10)\n",
    "# svd.fit(X)\n",
    "# print(svd.explained_variance_ratio_)\n",
    "# print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Individual Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = df_users.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_users)\n",
    "print(df_recipes)\n",
    "\n",
    "X = df_random.drop('rating', axis=1)  # training data\n",
    "y = df_random['rating']  # target values\n",
    "# TODO take into account df_recipes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)  # 67% training, 33% testing\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train)  # y_pred_train predicts X_train\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "#y_pred_train = np.rint(y_pred_train)\n",
    "#y_pred_test = np.rint(y_pred_test)\n",
    "\n",
    "#print(X_train)\n",
    "#print((y_pred_train))\n",
    "#print((np.array(y_train.to_list())))\n",
    "\n",
    "# TODO compare expected outputs with actual outputs\n",
    "print(y_pred_test)  # why float value ??\n",
    "print(y_test)\n",
    "\n",
    "# TODO solve\n",
    "print('Accuracy on training data =', metrics.accuracy_score(np.array(y_train.to_list()), y_pred_train))\n",
    "print('Accuracy on testing data =', metrics.accuracy_score(np.array(y_test.to_list()), y_pred_test))\n",
    "print('')\n",
    "print(metrics.classification_report(np.array(y_test.to_list()), y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_users['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_recipes_full = pd.read_csv('Data/RAW_recipes.csv')\n",
    "#Note that there are no duplicates\n",
    "df_recipes_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_recipes_full.drop('minutes', axis=1, inplace=True)\n",
    "df_recipes_full.drop('contributor_id', axis=1, inplace=True)\n",
    "df_recipes_full.drop('submitted', axis=1, inplace=True)\n",
    "df_recipes_full.drop('tags', axis=1, inplace=True)\n",
    "df_recipes_full.drop('nutrition', axis=1, inplace=True)\n",
    "df_recipes_full.drop('steps', axis=1, inplace=True)\n",
    "df_recipes_full.drop('ingredients', axis=1, inplace=True)\n",
    "df_recipes_full.drop('description', axis=1, inplace=True)\n",
    "\n",
    "df_recipes_full = df_recipes_full.rename(columns={'id': 'item'})\n",
    "\n",
    "df_recipes_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users_ratings = df_users.groupby(['user']).count()  # count the ratings for each user\n",
    "selected = users_ratings['rating'] > 30  # keep only 30 + ratings\n",
    "selected_users = users_ratings.loc[selected]\n",
    "random_selected = selected_users.sample(n=10)\n",
    "\n",
    "select_column_df = random_selected.reset_index()[\n",
    "    'user']  # reset_index() create a new index, and the userId became a column. Then, we can filter using the column name\n",
    "group_users = list(\n",
    "    select_column_df)  # iloc select by index, since our dataframe only has one row we read it from the index 0\n",
    "print(group_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group_ratings = df_users.loc[df_users['user'].isin(group_users)]\n",
    "total_recipes = set(df_recipes.index.tolist())\n",
    "num_ratings_df = df_users.groupby(['item']).count()\n",
    "considered_recipes = set(num_ratings_df.loc[num_ratings_df['user'] >= 30].reset_index()['item'])\n",
    "\n",
    "group_seen_recipes = set(group_ratings['item'].tolist())\n",
    "group_unseen_recipes = considered_recipes - group_seen_recipes\n",
    "\n",
    "print('Total amount of recipes,', len(total_recipes))\n",
    "print('Recipes that have at least 20 ratings,', len(considered_recipes))\n",
    "print('Recipes that have been rated by the currently selected group,', len(group_seen_recipes))\n",
    "print('New recipes that the group didnt try yet,', len(group_unseen_recipes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display\n",
    "import itertools\n",
    "from lenskit.algorithms import Recommender\n",
    "from lenskit.algorithms.user_knn import UserUser\n",
    "\n",
    "user_user = UserUser(15, min_nbrs=3)  # Minimum (3) and maximum (12) number of neighbors to consider\n",
    "recsys = Recommender.adapt(user_user)\n",
    "recsys.fit(df_users)\n",
    "group_unseen_df = pd.DataFrame(list(itertools.product(group_users, group_unseen_recipes)), columns=['user', 'item'])\n",
    "group_unseen_df['predicted_rating'] = recsys.predict(group_unseen_df)\n",
    "group_unseen_df = group_unseen_df[\n",
    "    group_unseen_df['predicted_rating'].notna()]  # remove the recipes we couldn't get a prediction for\n",
    "display(group_unseen_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Min-Max normalization of predicted_ratings\n",
    "\n",
    "maxVal = group_unseen_df['predicted_rating'].max()\n",
    "minVal = group_unseen_df['predicted_rating'].min()\n",
    "group_unseen_df['predicted_rating'] = (group_unseen_df['predicted_rating'] - minVal) / (\n",
    "        maxVal - minVal)  # Normalized to 0 - 1 scale\n",
    "group_unseen_df['predicted_rating'] *= 5  # Normalized to 0 - 5 scale\n",
    "\n",
    "display(group_unseen_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Least Misery strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "least_misery_df = group_unseen_df.groupby(['item']).min().reset_index()\n",
    "# TODO: Find name of recipe from the RAW data\n",
    "least_misery_df = least_misery_df.join(df_recipes_full['name'], on='item')\n",
    "items_lm = least_misery_df['item'].copy()\n",
    "healthy_lm = ApplyHealthFilter(items_lm)\n",
    "\n",
    "least_misery_df['healthy'] = least_misery_df['item'].apply(lambda x: 1 if x in healthy_lm else 0)\n",
    "least_misery_df = least_misery_df.sort_values(by=\"predicted_rating\", ascending=False)[\n",
    "    ['item', 'predicted_rating', 'name', 'healthy']]\n",
    "\n",
    "least_misery_df = least_misery_df[least_misery_df.healthy == 1]\n",
    "display(least_misery_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Most Pleasure strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "most_pleasure_df = group_unseen_df.groupby(['item']).max().reset_index()\n",
    "# TODO: Find name of recipe from the RAW data\n",
    "items_mp = most_pleasure_df['item'].copy()\n",
    "healthy_mp = ApplyHealthFilter(items_mp)\n",
    "\n",
    "most_pleasure_df['healthy'] = most_pleasure_df['item'].apply(lambda x: 1 if x in healthy_mp else 0)\n",
    "\n",
    "most_pleasure_df = most_pleasure_df.join(df_recipes_full['name'], on='item').reset_index()\n",
    "most_pleasure_df = most_pleasure_df.sort_values(by=\"predicted_rating\", ascending=False).reset_index()[\n",
    "    ['item', 'predicted_rating', 'name', 'healthy']]\n",
    "\n",
    "most_pleasure_df = most_pleasure_df[most_pleasure_df.healthy == 1]\n",
    "\n",
    "display(most_pleasure_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Approval Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group_unseen_temp_df = group_unseen_df.copy()\n",
    "group_unseen_temp_df['voted'] = group_unseen_temp_df['predicted_rating'].apply(lambda x: 1 if x > 3.5 else 0)\n",
    "approval_df = group_unseen_temp_df.groupby(['item']).sum()\n",
    "approval_df.drop('user', axis=1, inplace=True)\n",
    "approval_df['predicted_rating'] /= len(random_selected)  # Normalize rating\n",
    "# Only keep the items with maximum approval\n",
    "approval_df = approval_df[approval_df.voted == approval_df.voted.max()]\n",
    "approval_df = approval_df.sort_values(by=\"predicted_rating\",\n",
    "                                      ascending=False).reset_index()  # Get the best rated items with max approval\n",
    "approval_df = approval_df.join(df_recipes_full['name'], on='item')\n",
    "display(approval_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
